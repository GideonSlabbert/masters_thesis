%%
%%  Department of Electrical, Electronic and Computer Engineering
%%  MEng Dissertation / PhD Thesis - Chapter 3
%%  Copyright (C) 2011-2016 University of Pretoria.
%%

\chapter{METHODS}

\section{CHAPTER OVERVIEW}

This chapter will describe the process taken to develop a visual analytic system that will support the FDD process.

\section{CONCEPTUAL DESIGN}

The sources referenced in Chapter 2 describes various ways of how to approach FDD and what methods work best to structure the FDD process. A common idea or thread in most of these sources is the need for data manipulation or transformation. The key to obtaining an understanding that there is a problem and what is causing it is to  view data in a certain context. This context needs to be created by interacting with the data. 

Almost all of the literature sources highlight a need for a structured approach with logical steps that lead to a final goal. \cite{venkatasubramanian2003review} describes a four step transformation process that classifies each step as a different data space; measurement, feature, decision and class. \cite{pirolli2005sensemaking} proposes a sensemaking model with four key steps: Information gathering, representation of information in a model, development of insight through manipulation of the model and finally creation of knowledge through analysis. \cite{fayyad1996data} describes a five step process that includes data selection, processing, transformation, mining and finally interpretation or evaluation. \cite{mahyar2010closer} proposes a structured approach to visual analytics that focuses of problem definition, visualization, analysis and reporting.

Based on the findings of all these sources it is proposed to also divide a new FDD tool into four distinct steps. The first and most important step would be the analysis of raw data. Before any complex transformations can be done it is important to understand what data is available and what the limitations are thereof. The human brain is very good at recognizing patterns without necessarily quantifying the model that is used to identify the pattern. It is also important to understand the limitations imposed by the frequency of the data used. If the data is of a very low frequency it might not be feasible to identify faults that are caused by high frequency failures which is common when the failure is related to a instrument failure in industrial processes. There might also be differences in the frequency of different parameters in the data set. \cite{streicher2019plant} also highlighted the importance of of a fast Fourier transform (FFT) to identify the active frequency regions, multiple frequency regions of interest could indicate that there are multiple disturbances present with different dynamics.

Process Information Management Systems (PIMS) and Laboratory Information Management Systems (LIMS) collect and store time series data in vastly different frequencies. For example a process data point might be available at minute intervals but a critical process specification like a viscosity analysis might only be available every eight hours. Furthermore LIMS data might also pose a challenge in terms of time shifts. The manual nature of how some lab samples are collected might cause a time shift between when the sample was taken and when the analysis was completed. In practice it can be very difficult to ensure that this time shift is consistent and also how to determine what the exact time shift is.

The raw data should not only be time series data but any other data that can be used to create context. As indicated by \cite{thornhill2007advances} and \cite{yim2006using} any information regarding plant topology can provide valuable insight into the casual links between variables. This could potentially simplify any models developed and avoid nonsensical conclusions. Although the XML schema discussed by  \cite{thornhill2007advances} provides an exciting prospect in itself there is no open source version of such a application currently available. For this reason this specific schema will not be incorporated into the tool under development but the idea can still be used. Any visual or diagram of the process that generated the time series data must be included in the first step of the FDD process. Even if the casual links are specified manually this still improves greatly on assuming all parameters are correlated to each other. Another source of additional data that is available on modern control systems is event data. Event data is a valuable source fo meta data that could include alarm data, changes made to control system parameters by operators or any other predefined alerts that would be logged based on the state of a variable, i.e. the changing state of a binary variable to indicate the state of a electrical motor.

Another aspect of the first step in the proposed process is to include the ability to incorporate note taking into the raw data analysis. This ensures that any information gathered by one user can be built on by others to further the discovery process. As explained by \cite{mahyar2010closer} these notes can be in the form of findings and cues, where the findings can be as a result of calculations which might only be related to one of the following steps and cues can be anything that is useful, for example an observation of the casual link identified by using the process topology.

The final thread that must be woven into the entire process is the ability to interact with the data. As highlighted by [20] it is more important to have a highly interactive tool than a very accurate tool because this will encourage the engagement of the user. An engaged user will spend more time to guide the FDD process and provide valuable input into it.

The second step in the FDD process should be where the data is transformed in such a way as to highlight the most important information. This information would be the signs of a fault occurring or that a fault has resulted in some change in how the variables relate to each other. Data transformation and data mining is mentioned as key part of the study done by \cite{fayyad1996data} into knowledge databases. The study by \cite{venkatasubramanian2003review} calls this the feature space where certain features of the underlining data set are brought forward where they can be analyzed in more detail. \cite{pirolli2005sensemaking} describes this as representing information in a model, a model is something that is created to help in the understanding of how variables in a complex system are related to each other.

In this second step the most customization must be allowed to the user to specify new methods of transforming data. There will always be new and creative ways to digest data and reduce dimensionality. For the purposes of this demonstration the tool called FaultMap that was developed by \cite{streicher2019plant} will be used as the data transformation engine. This will demonstrate how to perform a FDD analysis of a data set by following the LoopRank algorithm to highlight faults and monitor how they develop over time.

The third step in the FDD process will be a space where the model or transformation developed in the second step can be modified or adjusted to test certain hypothesis. In this step we decide if the transformation process has identified a true fault. \cite{venkatasubramanian2003review} aptly calls this the decision space. \cite{pirolli2005sensemaking} describes this as development of insight through manipulation fo the model. Only when the user can see how the model changes can they truly start to internalize the effect of the model on the raw data and start to gain real understanding of what the effect of the transformation truly was. \cite{mahyar2010closer} describes this point as the analysis stage which ties in well with the idea of getting a better understanding of the end result.

The fourth and final step is called the classification space by \cite{venkatasubramanian2003review}, in this space knowledge is created through the analysis performed in the previous step. Interpretation and evaluation of the model is done and a final report is compiled of the findings of the study. This step can be very intricately intertwined with step three, but it is still important to distinguish this as the step where the final result of the FDD process is generated. This final result could then be documented and tested on a new data set or it could be used to improve any of the previous steps by modifying the transformation technique used in step two. Finally it could be used to allow the user to respond to fault conditions in the process and improve the process stability.

\section{INTERFACE EXPLORATION}

The method for choosing an interface has become a very interesting challenge in itself. In recent times the amount of tools developed for FDD analysis have grown exponentially. Commercially available tools are offered by almost every major player in the process control industry. Table \ref{fig:commercial tools} displays four such tools available that are specifically marketed to the process control industry.

\begin{center}
	\begin{table}[H]
		\caption{Commercial tool examples}
		\label{fig:commercial tools}
		\begin{tabular}{ 
				!{\vrule width 1.1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}}
			%p{8cm}!{\vrule width 1pt}}
			\noalign{\hrule height 1pt}
			\cellcolor[gray]{0.9} \textbf{Solution} &
			\cellcolor[gray]{0.9} \textbf{Company} &
			\cellcolor[gray]{0.9} \textbf{Key theme}
			\\ \noalign{\hrule height 1pt}
			Aspen MtellÂ® & Aspentech & Predict degradation and impending failure
			\\ \hline
			Control Performance Monitor & Honeywell & Condition-based maintenance
			methodology
			\\ \hline
			SAM GUARD & Precognize & Quickly and accurately identifies equipment failures
			\\ \hline
			AMS & Emerson & Prediction and protection for production assets
			\\ \noalign{\hrule height 1pt}
		\end{tabular}
	\end{table}
\end{center}
Although it is out of the scope of this work to evaluate each of these tools in detail it is clear that a lot of work is being done to use a data driven approach as described by \cite{cecati2015survey} to improve unit operations.

A bigger focus of this study is to define what is really important for someone using an FDD tool and how a tool should be created with the specific goals of the end user in mind. Very often end users experience frustration with existing tools because they are not flexible enough to cater for the unique needs of the individual. This is a common problem in any field but even more so in the process control industry due to the dynamic nature of processes and the infinite number of ways a complex industrial system can be configured. Every person has his or her own unique perception of how to visualize a process control problem and conduct FDD. In the engineering field a person will very quickly developed their own tool using easily accessible software platforms like Microsoft Excel to accomplish a certain analysis task.

As highlighted by \cite{saraiya2006insight} inferior tools in terms of visualization are preferred if they are more interactive. This interactivity can also be linked to the idea of flexibility. A more flexible environment will allow the user more freedom to express his or her own unique approach to a problem. There is also a limit to the amount of flexibility that should be given. Without a pre-defined model and structure the tool will not support the user in solving a problem effectively. An example of structure requirement could be the cells in an  spreadsheet, without the cells as a structure to let the user build on each successive calculation or output the user might lose track of what was done to get to the end goal.

This environment that the user of a system predominantly interacts with is also referred to as the front end of the system by \cite{Herbst2017}. \cite{Herbst2017} cited the ability to efficiently use the system without requiring new knowledge as one of the key goals of the Evert project. This goal will be analyzed in more detail because it is a key feature of FDD systems that often goes unnoticed but plays a very big role in designing how the system will work.

Three existing platforms were explored by \cite{Herbst2017} to determine their suitability for a FDD system specifically catered to time series data. These three tools were JupyterLab, Glue and Lyra. JupyterLab and Glue are both Python based platforms while Lyra is based on the Vega platform. The JupyterLab environment had key features that came very close to meeting the needs of Evert, it allowed for interactivity, a pluggable work environment and inline interactive graphs. One of the major drawbacks was the highly programming orientated working environment. It is very interesting to note that since the study of \cite{Herbst2017} has been completed in 2017 there have been multiple methods developed to get around this challenge which will be discussed in later sections.

A programming orientated environment could be seen as the most flexible environment. This would then allow the user to perform any FDD analysis but it would not become a FDD system if multiple users could not use it in similar ways to identify and diagnose faults. Some sort of structure is still required to guide the user in following the best thought patterns or series of steps described as a sensemaking model by \cite{pirolli2005sensemaking}.

One of the other key requirements of the Evert project as stated by \cite{Herbst2017} was to apply Python as the central programming language. A promising alternative platform that is also uses Python as the central programming language is called Plotly Dash. As indicated by \cite{ali2016big}, \cite{caldarola2017big} and \cite{Herbst2017} Plotly is a very effective visualization tool. Dash is a point and click interface built on top of Flask, Plotly.js, React and React Js which allows for the creation of interactive dashboards using pure Python. Dash is open source and creates these dashboards in applications to run on a web browser.

The Dash environment was explored as a possible FDD environment and visual analytic system. The end user will certainly not need to code to use the interface and development could be done in Python. A demonstration application was built that attempted to incorporate the basic elements needed to import time series data and start to explore in the data space as defined by \cite{venkatasubramanian2003review}. Dash provides very powerful tools that allow for a drag and drop interface to import new data and effortlessly visualize it in tables and figures. It was important that Dash had the functionality to allow the user to display and explore data in its raw tabular form inside a Dash Table. When the nature of the time series data, for example the sampling frequency or the time format, is not yet well understood this ensures that the users selects the correct processing techniques in the data manipulation steps.

Dash also had a very useful Store object that could be used to store data inside the web browser session after it was uploaded. This would make it easier to upload multiple data sets on the same application and navigate through data sets using tools like drop down menus. This also provided the flexibility to manipulate multiple data sets simultaneously, combine them or extract data from them. The Dash application provided all this flexibility in the session created on the users own web browser even if the application was hosted from a remote server. The data would not need to be transported back and forth between two locations which provided a performance benefit and also didn't need to be stored in a fixed location on the users work station.

The Dash Table and Figure objects were found to provide for a highly interactive environment. Based on the findings of \cite{mahyar2010closer} which advocates for the ability to incorporate user notes into a workflow an attempt was made to incorporate some sort of note taking functionality into the Dash application. A key element of making notes on time series trends would be the ability to indicate at what point in time the significant event occurred that that a note is referring to. A basic example of this would be a callout object on a spreadsheet graph like a speech bubble that would indicate to a point in time and also allow for text to describe the observation. This type of interactivity was not easily incorporated into a Dash Figure object. The interactive elements already present on a Dash Figure like the ability to pan or zoom make it difficult to assign a fixed location to a note object as described above. Another option would be to only include the pointer or indicator element in the graph object and add the free text information in a separate input field next to the graph. Although it is possible to include a indicator object on a graph before rendering the functionality to dynamically add these objects after rendering is not yet available. This difference means the user cannot assign markers to a graph while the data is being explored which would be the ideal setup for a note taking interface. Based on these observations the note taking functionality is not yet at a maturity level required for this type of analysis environment.

The development of the demonstration application also allowed for experience with debugging in the development space. Dash automatically creates a new user session every time the source code is modified, this saves a lot of time when making changes or adding new features. Challenges identified were in the application environment when a dynamic object was not behaving as intended or experienced a error that affected its rendering. These errors were not severe enough to prevent the application from running but created pop-up error messages in the application linked to the javascript element on which the platform is built. These errors were difficult to troubleshoot and the Dash environment is not widely adopted yet which limits the amount of online troubleshooting forums to utilize. A method is available to print data to the command prompt window from where the application is opened. This does allow for understanding data structures and dynamic outputs.

The development experience when trying to build a very interactive environment has the potential to become too time consuming which might discourage users from developing their own add ins or make changes to the environment. Together with the lack in interactive note taking functionality indicates that Plotly Dash is not the ideal platform for the intended goal of this project.

The interactive note taking ability is something that is almost built into the way in which Jupyter notebooks are created. Each cell in a notebook could be used for executing code or creating a note. The challenge of creating indicator objects and text notes on a interactive graph would still not be solved but a portion of the graph data that is important for reference to the note could easily be extracted and placed in a cell adjacent or below a note. The interactive Graph objects developed by Plotly could still be used inside this environment. Recent developments have also provided exciting possibilities for creating dashboards from Jupyter notebooks. New solutions like Panel or Voila can easily convert a Jupyter notebook with its interactive elements into a dashboard application. This provides the option of developing in a notebook environment which is very easy to debug or troubleshoot and then also display a dashboard environment for users that want to focus on only the rendered objects.





%% End of File.
