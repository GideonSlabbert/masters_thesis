%%
%%  Department of Chemical Engineering
%%  MEng Dissertation - Chapter 2
%%  Copyright (C) 2011-2016 University of Pretoria.
%%

\chapter{LITERATURE STUDY}

\section{CHAPTER OBJECTIVES}

This chapter explores literature in the field of fault detection and diagnosis of process control in industrial processes. It then elaborates on visualization techniques and how they could be used to aid in the fault detection and diagnosis process.

\section{FAULT DETECTION AND DIAGNOSIS}

The automated control of industrial processes has developed significantly in recent decades. Advances in computing power had allowed the distributed control system to become common place in most industrial processes. This together with advanced control techniques like model predictive control has allowed for automated real time optimization of the key economic variables. Despite these advances one of the most important tasks in managing a process is still being done manually by operational teams; responding to abnormal events. It can be described as the timely identification, diagnosis of the root cause and the appropriate supervisory action to address an abnormality and bring the process back to stability. \cite{venkatasubramanian2003review} defined this task as abnormal event management (AEM) which is critical to the stability of a process. \cite{venkatasubramanian2003review} refers to statistics that indicate that \SI{70}{\percent} of industrial accidents are caused by human errors. This indicates that methods to improve this task can significantly improve the economic, safety and environmental impact of these industries.

A fault can be described as a deviation of an observable variable or calculated parameter from its acceptable operating range \cite{venkatasubramanian2003review}. This can be a process deviation like low steam outlet temperatures from a boiler. A possible cause for this could be a failure on a attemperator valve which can be referred to as the basic event or root cause. The diagnosis task can be seen as a classification problem wherein a root cause or fault must be placed in a certain group of faults which will have a pe-determined method to solve them.

Disturbances in single control loops are fairly straight forward to solve but disturbances that propagate through a complex system on a plant wide scale are much harder to present a much bigger challenge. One of the goals of process control is to shift process variability to places like buffer tanks and plant utilities. In modern industrial processes these options have become limited as inventory is usually kept to a minimum and the use of recycle streams for heat integration has introduced extra sources complex interaction \cite{thornhill2007advances}.

When considering which type of disturbances to analyze, the time frame of the disturbance can be used to distinguish between different types of disturbances. \cite{thornhill2007advances} classified the types of disturbances in a plant wide setting into three main categories with regards to timescales:
\begin{enumerate}
	\item slowly-developing, i.e. fouling of a heat exchanger
	\item persistent and dynamic
	\item abrupt, i.e. a compressor trip
\end{enumerate}
The second item, persistent and dynamic disturbances that propagate through an entire system on a continuous basis is of interest for this study.

\subsection{FDD AS ANALYTICAL REDUNDANCY}

Fault diagnosis is normally used to improve the reliability of a system \cite{gao2015survey}. There are two methods that can be used to accomplish fault diagnosis: the first is hardware redundancy and the second is software or analytical redundancy. Hardware redundancy requires the use of identical components that use the same input signals to generate duplicate output signals. If the output signal then differs between the two redundant controllers it is easy to detect a fault and take action, examples of this method is limit checking and majority voting. Hardware redundancy can be very expensive and is often times not present due to cost considerations or only present on the most critical sensor elements. This creates the opportunity for faults to occur on any of the other non-redundant elements and then propagate through to even these redundant elements. For this reason analytical redundancy has become the main method for doing fault diagnosis. 

In this context of analytical redundancy, fault diagnosis includes three components; fault detection, fault isolation and fault identification \cite{gao2015survey}. Fault diagnosis starts with detecting that a fault has occurred, after detection the fault needs to be isolated by providing information as to where in the system the fault is located. Finally the fault identification step provides information on the types, shapes and size of the fault which basically provides degree/ level of malfunction.

\subsection{CLASSIFYING FDD METHODS}

\cite{gao2015survey} completed a comprehensive overview of fault diagnosis techniques that have been developed over the last 30 years with specific focus on highlighting new developments over the last 10 years. According to this study by \cite{gao2015survey}, fault detection and diagnosis techniques can be divided into three methods; model-based methods, signal-based methods and knowledge-based methods.

Model based methods use a model of the system derived from physical principles or system identification techniques. The system outputs are then compared to model predicted outputs to identify faults.

Signal based methods do not rely on input- output relationships but rather extract features from the signals measured in the system. Diagnostic decisions are then taken by comparing these key features to prior healthy versions of the same features. These methods can be time-domain based like computing the mean or standard deviation. They can also be frequency-domain based by looking at the spectrum of signals such as the discrete Fourier transform.

\cite{cecati2015survey} discussed the knowledge based methods in detail in the second part of the overview paper. Knowledge based methods are also referred to as data driven methods. These methods can be either qualitative or quantitative in nature. They require large amounts of historical process data and computing power to extract process knowledge. 

Qualitative knowledge based fault diagnosis is expert-system methods that rely on sets of rules to evaluate data and detect faults. The rules have to be developed specifically for the system in question but integrating this method with a object-orientated paradigm provided more flexibility by developing more general rules. Another method used with this description is qualitative trend analysis (QTA) which identifies process trends from noisy data and compares them to a database that contains fault trends. The QTA method has been advanced by combining it with integrated signed directed graphs (SDG) to compensate for some of the disadvantages of this method.

Quantitative knowledge based methods are based on solving the pattern recognition problem. This can be done using statistical analysis techniques like principal component analysis (PCA), partial least squares (PLS), independent component analysis (ICA) or support vector machine (SVM). This can also be done using Non statistical analysis based methods which are better suited for non-linear applications, an example of this is a neural network.

A lot of new research is being done in quantitative knowledge based methods due to the vast amount of data that is now available in modern industrial complexes. \cite{jiang2019review} defines these methods as multivariate statistical process monitoring (MSPM). For linear Gaussian process methods such as PCA, PLS and canonical correlation analysis CCA are used. To cater for non-Gaussian, non-linear and dynamic characteristics numerous extensions have been developed for the above mentioned methods.

\subsection{EVALUATING FFD METHODS AND STRUCTURE}

Two very important concepts to understand when evaluating fault FFD methods is completeness and resolution \cite{venkatasubramanian2003review}. Completeness describes how well a method is at detecting all the possible faults exhibited by the system. Resolution then relates to how well the method is at classifying the fault into a small subset of the available faults. A diagnostic system has the following desirable characteristics according to \cite{venkatasubramanian2003review}.
\begin{enumerate}
	\item Quick detection and diagnosis - creates a trade-off between robustness and performance
	\item Isolability - ability to distinguish between different failures
	\item Robustness - robust to various noise and uncertainties
	\item Novelty identifiability - can detect a new/novel fault in the process
	\item Classification error estimate - error measures to project confidence levels
	\item Adaptability - be able to develop new scope as new problems/faults arises and more information becomes available
	\item Explanation facility - provide explanation of how fault originated and propagated. Ability to reason cause and effect relationships in the process.
	\item Modeling requirements - modeling effort required should be minimal
	\item Storage and computational requirements - achieve balance between computational complexity and storage requirements
	\item Multiple fault identifiability	
\end{enumerate}

In any diagnostic system the measurements and data goes through a transformation process. Two important components to take note of when considering transformations are the a priori process knowledge and the search technique used \cite{venkatasubramanian2003review}. In general the transformation of data will go through four steps:
\begin{enumerate}
	\item Measurement space
	\item Feature space
	\item Decision space
	\item Class space	
\end{enumerate}

\cite{venkatasubramanian2003review} explains these spaces as follows:
The measurement space contains the raw input signals to the diagnostic system. The feature space contains a set of variables that are derived from the measurement space using a priori problem knowledge. Useful features are extracted, in this transformation there is often a significant dimension reduction. This is normally done by using feature selection or feature extraction. Its assumed that features cluster better than measurements which will facilitate improved classification or better discrimination. By extracting discriminant features it takes the burden of the next transformation so that it can easily find the features distinguish between features that match certain decisions.
The mapping from feature to decision space is usually done via some sort of objective function or using simple threshold functions. The class space consist of the list of possible faults and the decision space maps to it via threshold functions, template matching or symbolic reasoning. This could be summarized as using some sort of search or learning algorithm to complete these mappings. It is also the final interpretation of the diagnostic system delivered to the user. The decision space and the class space usually have the same dimensions. This could provide the option of combining these two spaces but the advantage of still keeping them separate is that a certain diagnostic method might not be able to give a crisp solution as to what fault is being detected.

A priori knowledge plays a crucial role through this entire transformation process, it can greatly simplify the problem in every step of the process. Sets of failures and relationships between observations and failures is crucial to the success of any method. This relationship can be inferred, the most basic example begin a lookup table or it can be based on domain knowledge. First principle knowledge is referred to as deep, casual and model-based knowledge whereas knowledge gained from past experience with the process can be referred to as shallow, compiled, evidential or process history-based knowledge \cite{venkatasubramanian2003review}.

\subsection{NEW APPROACHES TO DATA DRIVEN METHODS}

In the following sections of this exploration more focus will be given to knowledge/ data driven methods. Improvements in computing power and better access to tools used for analyzing data make it possible to significantly improve the contribution of data driven methods to the FDD process.

New data driven methods that apply concepts from other fields have started to show some promising results. One of these methods is the use of the PageRank algorithm \cite{bryan200625} employed by google on the ranking of control loops based on the degree of interaction.\cite{farenzena2009looprank} has developed a methodology called LoopRank with the aim of prioritizing control loop maintenance by highlighting loops that have a higher correlation with other loops.\\

\subsection{PAGERANK DERIVATION}

The ranking algorithm developed by \cite{bryan200625} started with the idea of ranking web sites depending on how many times they are referenced by other websites.

Consider a group of 4 websites that contain references to each other in the manner indicated in Figure \ref{fig:7}. References are indicated by arrows.\\
\begin{figure}[H]
	\centering\epsfig{figure=webpages.PNG,scale=0.5}
	\caption{web pages with references}\label{fig:7}
\end{figure}
An initial idea would be that a page that has more references or back links is more important. Let $x_{k}$ be the importance of page k. If this method is followed then $x_{3} = 3$ would be the most important web page because it has the most references.

This initial ranking method indicates that $x_{3}$ is important, but it doesn't take into account the importance that $x_{3}$ passes on to $x_{1}$ because it is the only page referenced by $x_{3}$.

Another approach would be to let back links from important pages count more than back links from less important pages. If a page k has a back link from page j, then the score given to this back link can be calculated as $x_{k} = \frac{x_{j}}{n_{j}}$ where $n_{j}$ is the total number of references made by page j. The total score for page $x_{k}$ is then calculated by the sum of all back links to it adjusted for the number of references made. Applying this new method the importance score for each node is calculated:
\newline $x_{1} = \frac{x_{3}}{1} + \frac{x_{4}}{2} $ 
\newline $x_{2} = \frac{x_{1}}{3} $
\newline $x_{3} = \frac{x_{1}}{3} + \frac{x_{2}}{2} + \frac{x_{4}}{2} $
\newline $x_{4} = \frac{x_{1}}{3} + \frac{x_{2}}{2} $
\newline These equations can be written in a matrix form, which will be defined as the link matrix A:
\newline 
\[A=\begin{bmatrix}
0           & 0           & 1 & \frac{1}{2} \\
\frac{1}{3} & 0           & 0 & 0           \\
\frac{1}{3} & \frac{1}{2} & 0 & \frac{1}{2} \\
\frac{1}{3} & \frac{1}{2} & 0 & 0
\end{bmatrix}\]
\newline Each row in the matrix A represents a page or node while each column  represents the importance score from another node. For example row one represents node 1 and it has a back link from node 3 with an importance score of 1 and a back link from node 4 with a importance score of $\frac{1}{2}$.

if $x = \begin{bmatrix} x_{1} & x_{2} & x_{3} & x_{4} \end{bmatrix} ^{T}$ then the problem can be defined as the eigenvector problem:
\newline $Ax = x \lambda$ 
\newline with x being an eigenvector for A with en eigenvalue of 1
\newline if A is a $n\times n$ matrix, then a nonzero vector x is an eigenvector if $Ax = \lambda x$ for some scalar $\lambda$. The eigenvector x that solves this equation is then calculated to be:
\newline $x \approx \begin{bmatrix} 0.387 & 0.129 & 0.290 & 0.194\end{bmatrix}^{T}$
which shows that node one is in fact more important than node three.

\subsection{LOOPRANK ADAPTION}

\cite{farenzena2009looprank} developed a method called LoopRank which is based on the PageRank algorithm to prioritize control loop maintenance. Modern industrial sites have a large number of control loops which becomes challenging to maintain effectively given limited technical resources. 

The LoopRank method can be summarized in five steps:
\begin{enumerate}
	\item Collect time series data of control loop parameters
	\item Calculate the partial correlation between all the variables in the data set. This information is used to build a relative weight matrix A or link matrix as referred to in the PageRank algorithm
	\item Compute the LoopRank based on the relative weight matrix A using the eigenvector centrality theorem
	\item Multiply the result for each loop with a pre-defined weight that is based on the criticality of the loop. This weight is assigned heuristically depending on loop type and profitability
	\item Normalize the results to express the score for each loop as a percentage
\end{enumerate}

\cite{farenzena2009looprank} demonstrated the efficacy of LoopRank by testing it on an industrial case study in which it successfully flagged the most critical control loops.

\subsection{LOOPRANK DEVELOPMENT}

\cite{rahman2010new} compared LoopRank with a more traditional data driven method that utilizes the integral of absolute or squared error to determine loop interaction. Canonical correlation was used to calculate the relative weight matrix in the LoopRank methodology. It was found that both methods delivered similar results.\\ 

The LoopRank methodology was further expanded on by \cite{streicher2014eigenvector} with the goal of improving the relative weight matrix used in step 2 of the procedure as outlined by \cite{farenzena2009looprank}.\\

\cite{streicher2014eigenvector} demonstrated how transfer entropy provides a more robust estimation of the casual links between nodes in a relative weight matrix. This method was tested using data from the well known Tennessee Eastman case study and was able to successfully highlight the disturbance variables as the top ranking nodes in the matrix.

\cite{streicher2019plant} further developed the LoopRank methodology to calculate importance scores for multiple overlapping time regions. Studying how the importance scores change over time then provided a very useful method for performing incipient fault detection.  \\ 

\section{VISUALIZATION}

The methodologies involved in fault detection and diagnosis of industrial processes rely heavily on the visual analysis of time series data. Data is continuously generated by industrial processes, this data in its simplest form can be floating point or boolean values on a control system interface screen. This data is used by plant personnel as feedback on the current status of automated processes.

This study will focus on methods and approaches taken to visualize temporal or time series data. 

\cite{few2009now} described visualization as the graphical representation of information where data visualization allows for exploration and communication. \cite{telea2014data} added that the purpose of visualization is to gain insight into some process represented in the form of quantitative data by means of interactive graphics. Insight is defined by \cite{telea2014data} as reference to two types of information; answer questions about data that describes a particular problem and facts about the problem that the user might not have been aware of.

Some of the first books to discuss information visualization date back to 1967 with the publication of Jacques Bertin's Semiologie Graphique \cite{rees2019survey}. A survey by \cite{rees2019survey} highlighted the number of visualization books have been published each year, displayed in \ref{fig:1}.

\begin{figure}[!ht]
	
	\centering\epsfig{figure=Visualization_books_published.PNG}
	\caption{Visualization books published each year}\label{fig:1}
	
\end{figure}

The IEEE Visualization conference series aimed at sharing developments in this field started in 1990 and has contributed a large amount of data on how visualization assists other fields to better understand their data. \cite{isenberg2016vispubdata} compiled a database of these publications to assist researchers in this field to understand the history of visualization in a field and make the researcher aware of new developments trends. The papers presented through these conferences serve as valuable source to describe previous work related to visualization for the use of fault detection and diagnosis.

\subsection{VISUAL ANALYTICS}

Before designing it is important to understand the mental processes behind data analytics that are used by people to understand and gain insight into data. The work discussed in this section first attempts to describe the human aspect of of the process and then attempts to integrate the role of the machine or computer into the process.

One of the earliest  models for that describe the process of 'sensemaking' was developed by \cite{pirolli2005sensemaking}. In this model intelligence analysis or sensemaking  consists of:
\begin{enumerate}
	\item Information gathering
	\item Representation of the information in the form of an outline or model that aids the analysis
	\item Development of insight through manipulation of the outline or model
	\item Creation of a knowledge product or direct action based on the analysis
\end{enumerate}
The process is visually described by Figure\ref{fig:3}.
\begin{figure}[!ht]
	\centering\epsfig{figure=Sense_making_model.PNG,scale=0.5}
	\caption{\cite{pirolli2005sensemaking} sensemaking model}\label{fig:3}
\end{figure}

The rectangles represent the flow of data and the circles represent the process flow. What is very clear from this model is that there are a lot of back loops and sets of activities that cycle around finding information and making sense of the information. The end result of the process is the generation of novel information by the analyst.

The External data sources are filtered into a smaller data set called a shoebox. Snippets are then extracted from the shoe box based on observed relationships and placed in the Evidence File. A Schema or model is then developed and verified using the Evidence file. A Hypotheses is then form based on this model with a feedback loop that search for additional support of this hypotheses. Finally there is a presentation that is first reevaluated before becoming the product of this work. This model can be used in a top-down (theory to data)  or bottom-up (data to theory) approach.

Another approach that represents a more non-linear model that might be a closer representation to how humans actually process information. \cite{klein2006making} present a model that starts of with the assumption that people always start making sense of something with some perspective, viewpoint or framework. The frame basically functions as hypothesis of how the data fits together.

These frameworks or frames actually define what counts as data and shape the data. Frames also change as we acquire data making it a two way path between a frame and data.

The first cycle that starts from the frame/ data idea is the elaboration cycle. In this cycle the frame is expanded and questioned - there could be doubt about the explanation it provides. If the doubt, possibly caused by troublesome data is explained away then the frame is preserved.

The second cycle involves a re framing process. If questioning the frame leads to its rejection, it might lead to its replacement with a better one. We could compare frames to see which one fits the data best. 

This model is displayed in Figure\ref{fig:4}.
\begin{figure}[!ht]
	\centering\epsfig{figure=Frame_data_sensemaking_model.PNG,scale=0.7}
	\caption{\cite{klein2006making} frame-data sensemaking model}\label{fig:4}
\end{figure}

This is a continuous process in which a frame is strengthened or improvement at each iteration.

Any method for turning data into knowledge requires the manipulation of the data. The analysis would start with an analysis of the data. This analysis has traditionally been done manually but with the exponential growth in the amount of data that can form part of an analysis other methods are being developed to make this step more efficient. \cite{fayyad1996data} analyzed this challenge in the emerging field of knowledge discovery in databases (KDD). In simple terms this field is concerned with development of methods and techniques for making sense of data.


One of the most noteworthy fields where these methods are applied is in the areas of astronomy. These fields contain terabytes of data contained in images. Although industrial application of data analysis contain a lot less data than this it could still be worth while studying these techniques to see if they can assist with effectively utilizing large amounts of time series data.

The goal of KDD is extracting high-level knowledge from low-level data in the context of large data sets \cite{fayyad1996data}. The KDD process is visually described by Figure \ref{fig:6}.
\begin{figure}[!ht]
	\centering\epsfig{figure=knowledge_discovery_in_databases.PNG,scale=0.8}
	\caption{\cite{fayyad1996data} steps in the knowledge discovery in databases model}\label{fig:6}
\end{figure}
The process is described by the following steps:
\begin{enumerate}
	\item Develop understanding of application domain and use prior knowledge to determine the goal of the KDD
	\item Create a data set or subset of the available data that will be used to discover useful information
	\item Data cleaning, in this step invalid data like noise of questionable data is removed in order to prevent false deductions
	\item Data reduction and projection - finding useful features to represent the data, in this step dimensional reduction can be done to simplify the analysis
	\item Match the goals of KDD to a specific data mining method
	\item Exploratory analysis of the model and hypothesis selection, in this step the data mining method is chosen as  well as the methods that will be used to search for patterns
	\item Data mining - searching for patterns in the data that would confirm the hypothesis
	\item Interpreting the mined patters
	\item Acting on the discovered knowledge, using it to solve the problem
\end{enumerate}
These steps describe a linear process but there can be movement back and forth between the steps to refine the process.


\cite{sacha2014knowledge} goes a step further to describe the process in which knowledge is generated from data by developing a model that defines how the human and computer interact. 
This model is split into two parts: 
\begin{enumerate}
	\item computer system which involves the data, visualization and analytic models
	\item human component that describes the cognitive process  associated with an analytical session
\end{enumerate}
What makes visual analytics so effective is the integration of these two parts; the human side  provides the perceptive skills, cognitive reasoning and domain knowledge while the computer side provides the computing data storage capability \cite{sacha2014knowledge}. This visual analytic model also attempts to incorporate some of the other models described earlier like the Sensemaking model \cite{pirolli2005sensemaking} and the Knowledge Discovery Process in Databases (KDD) \cite{fayyad1996data}. The complete knowledge generation model with the reference to other models is displayed in Figure \ref{fig:5}.
\begin{figure}[!ht]
	\centering\epsfig{figure=human_computer_va_model.PNG,scale=0.5}
	\caption{\cite{sacha2014knowledge} computer-human interaction in knowledge generating process}\label{fig:5}
\end{figure}


The computer part of the model has three main elements:
\begin{enumerate}
	\item Data - this includes all the data that describes the analytical problem and includes facts in any structured, semi-structured or unstructured manner. Another element to this is a term called meta data, which could be data that describes the structure of other data or data that summarizes other data.
	\item Model - This can be any descriptive measure of the data ranging from statistical properties of the data to complex data mining algorithms. This process can deliver a single number to describe the strength of an hypothesis or a complex pattern.
	\item Visualization - In this process the data is transformed into some kind of visual artifact that enables the analyst to detect relationships.
\end{enumerate}, 
The human element is also described in three main loops:
The computer part of the model has three main elements:
\begin{enumerate}
	\item Exploration loop - Findings are made based on observations of the visual analytics system, Actions or individual tasks are then performed that result in tangible  and unique responses from the visual analytics system. 
	\item Verification loop - Insight is gained from interpreting Findings, often with previous domain knowledge to generate units of information. A Hypothesis is then formed based on these Insights and tested in the visual analytics system through Actions.
	\item Knowledge - The final part is to build up confidence in the Insights from the data and Hypotheses tested on the data. If  patterns or stories told by the data makes sense and can be used to answer the questions from the visual analytic process then Knowledge is generated. This Knowledge is added to the domain knowledge of the analyst to ask more questions and generate more Knowledge.
\end{enumerate}
This model proposed by \cite{sacha2014knowledge} again highlights the iterative process of visual analytics and how feedback loops are used in every element of the model.

\subsection{VALUE OF RECORD KEEPING IN VISUALIZATION}

\cite{mahyar2010closer} conducted a experiment to study the visual analytics process and the role of record keeping in this process. This study also elaborated on key elements for the design of a collaborative visual analysis tool. The study required groups to complete tasks given a certain data set using computer aided visual analytics.
Actions taken during visual analytics were grouped into four phases:
\begin{enumerate}
	\item Problem definition - understanding what needs to be solved
	\item Visualization - generating visual artifacts
	\item Analysis - most complex phase that requires examining of charts and making sense of them combined with other data
	\item Dissemination - compiling findings in a report
\end{enumerate}
The findings from the study conducted by \cite{mahyar2010closer} confirmed observations by other researchers about the non-linear temporal order of activities. While completing the tasks users would move through the phases in a variety of orders. The visualization and analysis phases were strongly interrelated, the users moved back and forth between these two phases much more than the other phases.

\cite{mahyar2010closer} generated a very visual model of these four phases with key elements noted in each phase:
\begin{figure}[!ht]
	\centering\epsfig{figure=Visual_Analytics_Model.PNG,scale=0.5}
	\caption{Visualization books published each year}\label{fig:2}
\end{figure}

Record keeping involves the saving of visual artifacts and notes. In most cases a visual artifact can convey a complex idea more effectively than notes. Visual artifacts were saved for two main reasons; using them in reporting and using them to enhance the analysis process.\cite{mahyar2010closer} recommends that visual analytic systems provide functionality to save visual artifacts like charts for later use and comparison. Note taking also plays a critical role in all phases of the process. The notes high level content could be divided into two categories; findings and cues. Findings are a results of mathematical calculations, statistical operations or decisions and outcomes of the analysis process, they could also take to form of saved charts. Cues can be anything that is not directly extracted from the visualizations that could assist the user to framing or understanding the task at hand. A larger proportion of findings were recorded in the analysis phase while the cues were largely taken during the visualization phase. The study concluded that record keeping is used intensively by data analysts.

\cite{saraiya2006insight} completed a longitudinal study of a bioinformatics data set analysis to gain understanding of the entire analysis process from raw data to insights. Visual analytics can be described as a process wherein user gain insight into complex data. The study highlighted how analysts gather and build insights over long periods of time and connecting the data to extensive domain knowledge and expertise. It is emphasized that analyst spend a significant amount of time on manual manipulation of data and this creates a very important need for the visualization tool to be interactive. Inferior visualizations with interaction were preferred over superior static visualizations.

\subsection{VALUE OF RECORD KEEPING IN VISUALIZATION}

Practical side - how long does each step in analysis take, machine liearning, 60 percent of time taken to clean data

Visualization critical for outleir detecton - visual anomoly detection system yet to be beaten

\subsection{VISUALIZATION TOOLS}

\cite{barnard2015usability} performed a comparison of visualization tools in the development of a tool that can be used for data analysis at CERN. The goal of this tool was to enable scientist to observe a data set as a whole in order to make better decisions of what variable pairs and time frames to analyze during test campaigns.

JavaScript and Python Libraries were investigated and evaluated. D3 and Dygraphs were both discussed as JavaScript libraries that can be used for plotting. Matplotlib was discussed as a Python library that is based on the plotting functionalities of MATLAB. Two other Python libraries, Bokeh and Mpld3 were discussed with specific attention to their capabilities to generate HTML which enables them to easily present graphs on a web browser. Furthermore Mpld3 can integrate Matplotlob plots using D3 to render them to a web page.

\cite{barnard2015usability} did a detailed comparison of Bokeh and Mpld3 which identified Bokeh as their preferred choice. Comparing key aspects like loading time, rendering time and features Bokeh was faster and had more actively developed libraries.

\cite{fahad2018big} did a comparison of data visualization systems with the specific focus on big data applications. Tools were categorized into coding and zero coding tools with graphical user interfaces. 

Within the coding category two languages were discussed; Python and R. Within Python multiple libraries were analyzed for visualization, including Bokeh, Altair, Seaborn, ggplot and Pygal.
The zero coding category included Tableau, Infogram, Charblocks, Datawrapper, Plotly, RAW and Visual. It's important to note that of these zero coding tools Plotly is also a Python library used by the data science community.

\cite{ali2016big} also completed an overview of tools used for big data visualization and the challenges faced with attempting to visualization large amounts of data. The tools were compared using the following criteria:
\begin{enumerate}
	\item Open Source
	\item Integration with popular data sources like Hadoop Hive or Google Analytics
	\item Interactive visualization
	\item MOOCS; tutorials available for online learning of the tool
	\item API; can the services of the tool be embedded
\end{enumerate}
Five tools were analyzed using this criteria:
\begin{enumerate}
	\item Tableau
	\item Power BI
	\item Plotly
	\item Gephi
	\item Excel 2016
\end{enumerate}
A summary of the results is displayed in 
Table \ref{fig:tool_comparison} shows an example of a table.
\begin{center}
	\begin{table}[H]
		\caption{This is a table caption}
		\label{fig:tool_comparison}
		\begin{tabular}{ 
				!{\vrule width 1.1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}}
				%p{8cm}!{\vrule width 1pt}}
			\noalign{\hrule height 1pt}
			\cellcolor[gray]{0.9} \textbf{} &
			\cellcolor[gray]{0.9} \textbf{Open source} &
			\cellcolor[gray]{0.9} \textbf{Integration} &
			\cellcolor[gray]{0.9} \textbf{Interactive} &
			\cellcolor[gray]{0.9} \textbf{MOOCS} &
			\cellcolor[gray]{0.9} \textbf{API}
			\\ \noalign{\hrule height 1pt}
			Tableau & N & Y & Y & Y & Y
			\\ \hline
			Power BI & N & Y & Y & Y & Y
			\\ \hline
			Plotly & Y & N & Y & Y & Y
			\\ \hline
			Gephi & Y & N & Y & Y & Y
			\\ \hline
			Tableau & N & Y & Y & Y & Y
			\\ \noalign{\hrule height 1pt}
		\end{tabular}
	\end{table}
\end{center}

One of the biggest challenges with modern visualization tools is the latency created by processing large data sets \cite{ali2016big}. This can be addressed by using pre-computed data, paralyzing data processing and rendering and using predictive middelware.

\cite{caldarola2017big} completed a comprehensive survey of tools used in Big Data applications. The concept of Big Data has different dimensions, one model that attempts to described these dimensions is widely known as 3Vs. The 3Vs stand for Volume, Velocity and Variety.  Each of these dimensions pose its own unique challenges in the data management process.

The analysis and visualization of data was represented in a diagram that distinguishes between four different types of software solutions \cite{caldarola2017big}. The diagram displayed in Figure \ref{fig:10} depicts the different types according to the Volume and Variety challenges in the 3Vs model. 
\begin{figure}[H]
	\centering\epsfig{figure=big_data_visualization_categories.PNG,scale=0.9}
	\caption{Big data visualization solutions}\label{fig:10}
\end{figure}
Differentiation is also done according to four main categories that describe the goals of visualization. The first goal would be the clear and effective communication of information to the user with the creation and study of the visual representation of data. The Information Visualization category is created to describe this goal. The second goal diverges from the first one in terms of spatial representation. If spatial representation is given to the visualization because of the intrinsic spatial layout of data like a flow simulation in a 3D space, then this can be seen as Scientific Visualization. These two categories can be further divided according to the dimension of volume and variety of data. In the end these four categories chosen by \cite{caldarola2017big} to divide software are:
\begin{enumerate}
	\item business intelligent tools for data mining of heterogeneous data
	\item scientific visualization tools focused that focus on specific spatial representations using larger data sets
	\item data visualization tools used for visual exploration
	\item information visualization tools used for interactive visual exploration of data
\end{enumerate}
\cite{caldarola2017big} analyses 35 different software solutions in these four categories. What is interesting to note is that Plotly is well described in both the data and information visualization categories which implies that does well at handling big data challenges with variety and volume.

\cite{Herbst2017} completed a study of information visualization theory and developed a time series analysis tool. The goals of the visualization tool developed by \cite{Herbst2017} was called Evert, it had the following goals:
\begin{enumerate}
	\item Interactive visualization that enables exploration of time-series data
	\item Seamless operation across all major operating systems
	\item Python used as central programming language
	\item Allow for expansion of the tool through plugins
	\item User can efficiently apply the tool in their environments without the need to acquire new knowledge
\end{enumerate}
Three existing visualization environments where also analyzed to determine of they could be used as the platform for this design. The environments investigated were JupyterLab, Glue and Lyra. JupyterLab was designed to be the future of the Jupyter project's notebook interface. The JupyterLab work environment consists of multiple windows of Python interpreters. Glue or GlueViz is a Python-based project designed for multidimensional data analysis. A GUI is launched when the program is activated through which a user would interact with the tool. Lyra is a tool based on the Vega platform that doesn't require any coding to generate data visualizations. Althoug all three of these tools have their own merits they were not chosen as the basis for the Evert project. The design of JupyterLab was too linear and at the time could not allow for interactive visualizations accross interpreter windows. Glue couold not run on a web based server and could not allow for the customization of its interface. Lyra didn't allow for data manipulation and didn't support plug-ins.\\

During the development of Evert one of the key criteria in deciding what interface to use was the rendering of visualization displayed in a browser. Two main approaches are followed in this regards; Canvas and Scalable Vector Graphics. Canvas is a graphics API developed in HTML5 which generates pixels to represent the image. Scalable Vector Graphics (SVG) is an XML-based image format used to define two dimensional vector based graphics. \cite{Herbst2017} highlighted the flexibility of SVG elements and speed of rendering as the motivation for using this method in the final design. The use of D3 Javascript library as a standard for creating SVG visualizations in a web environment prompted  \cite{Herbst2017} to investigate the following four plotting libraries:
\begin{enumerate}
	\item Plotly.js
	\item C3.js
	\item DC.js
	\item MPLD3
\end{enumerate}
Plotly was chosen as the preferred visualization library due to its superior features such as pan, zoom and data selection interaction and ability to generate more than 2 Y-axis.


\section{VISUALIZATION AND FDD}

\cite{thornhill2007advances} described the exiting potential of combining data driven methods with process information to enhance the diagnosis step. The process information required is a qualitative model of the process topology which captures the logical pathways that information can flow. A standard called Computer Aided Engineering Exchange (CAEX) specifies an XML schema that can to capture the process topology and feed it into a data driven tool.\\

\cite{yim2006using} demonstrated a tool that combined plant topology information written in XML with results form a signal analysis tool called Plant-Wide Disturbance Analysis (PDA) to deliver enhanced isolation and diagnosis of root causes. The connectivity information provided by the plant topology allows the tool to verify or reject hypothesis about the propagation path of a fault. The tool described here was built in C\# and contains a reasoning engine that can searches for physical paths and determines root causes for measured plant-wide disturbances.\\

Another useful model to visualize process connections is the signed directed graph (SDG) model. This model provides a useful way to visual cause-effect relations in continuous systems \cite{yang2010qualitative}. The process variables are denoted as nodes and the casual relations as directed arcs. The sign of the arc indicates the casual flow of the process. \cite{streicher2019plant} used SDG models to aid in the logical reasoning process and display the results obtained from data driven methods like the LoopRank algorithm that was expanded on in earlier sections.\\

Process monitoring and fault detection of batch processes has received attention in recent years. The extensive use of batch processing in the production of high value products makes it a important to ensure that batch runs are completed with a high level of success. \cite{wang2018geometric} developed a novel approach to process monitoring and fault detection in batch processes by utilizing a technique they termed time-explicit Kiviat diagrams. The core idea behind this technique is based on Multi-way Principle Component Analysis that facilitates batch-wise unfolding of the three dimensional (Time*Variables*Batches) batch data matrix into a two dimensional matrix that captures the variation across batches. The type of data required to warrant the use of this technique is very similar to the data generated by multiple time region analysis in the LoopRank analysis done by \cite{streicher2019plant}. Each batch can be seen as a time region in the LoopRank analysis and the analysis can be seen as the completion of multiple batch runs.

SDG models can be obtained by flow sheets, empirical knowledge and mathematical models but in general they are more often derived from qualitative process knowledge and experience \cite{yang2010qualitative}. These models are of particular importance in plant-wide fault detection because they can help explain the fault propagation through a complex system. The directed graph description can also be represented in a matrix form an adjacency matrix with each element containing a 1 or 0 denoting the direction of the interaction between two nodes.

\cite{streicher2019plant} combined the application of the LoopRank algorithm and enhancements in the casual relationships identified into a tool that has the ability to test various different methods used in FDD. The tool is predominantly developed in python with Java dependencies to perform the transfer entropy estimations used in the interaction matrix. The tool makes use of various graphical models to display information ranging from basic trending of how importance scores vary across time regions to intricate SDG graphs displaying fault propagation paths.


%Equation \eqref{fig:example} shows an example of a numbered equation.
%\begin{equation}
%  \Delta w_{i,j} = \alpha a_i \Delta[j]  \label{eqn:example}
%\end{equation}
%
%\begin{align}
%  \m{QX} \m{a} & = \m{Qy} \notag \\
%  \therefore \quad (\m{QX})^T \m{QX} \m{a} & = (\m{QX})^T \m{Qy} \notag \\
%  \therefore \quad \m{a} & = \left( (\m{QX})^T \m{QX} \right)^{-1} (\m{QX})^T \m{Qy} \notag \\
%        & = \left( \mt{X} \mt{Q} \m{QX} \right)^{-1} \mt{X} \mt{Q} \m{Qy} \notag \\
%        & = \left( \mt{X} \m{W} \m{X} \right)^{-1} \mt{X} \m{W} \m{y}
%          \label{equ:weightedleastsquares}
%\end{align}

%\subsection{A subsection with a long heading that continues from the
%  first line onto the second line see also table of contents}
%
%\subsubsection{Testing deeper nested section headers}

%\ref{fig:examplea} and \ref{fig:exampleb}.
%\begin{figure}[!ht]
%  \begin{minipage}[b]{.5\linewidth}
%  \centering\epsfig{figure=figure1.eps}
%  \subcaption{A subfigure}\label{fig:examplea}
%  \end{minipage}%
%  \begin{minipage}[b]{.5\linewidth}
%  \centering\epsfig{figure=figure1.eps,angle=-90}
%  \subcaption{Another subfigure}\label{fig:exampleb}
%  \end{minipage}
%  \caption{This is a figure caption. In the example \subref{fig:exampleb}
%  is a rotated version of \subref{fig:examplea}.}\label{fig:example}
%\end{figure}



%% End of File.
