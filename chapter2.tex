%%
%%  Department of Electrical, Electronic and Computer Engineering
%%  MEng Dissertation / PhD Thesis - Chapter 2
%%  Copyright (C) 2011-2016 University of Pretoria.
%%

\chapter{LITERATURE STUDY}

\section{CHAPTER OBJECTIVES}

This chapter explores the basic 

\section{FAULT DETECTION AND ANALYSIS}

New data driven methods that apply concepts from other fields have started to show some promising results. One of these methods is the use of the PageRank algorithm \cite{bryan200625} employed by google on the ranking of control loops based on the degree of interaction.\cite{farenzena2009looprank} has developed a methodology called LoopRank with the aim of prioritizing control loop maintenance by highlighting loops that have a higher correlation with other loops. In this methodology a relative weight matrix is compiled using partial correlation between input and output variables. The importance scores of each variable is then determined by calculating the normalized eigenvector of the relative weight matrix.\\ 

\cite{rahman2010new} compared LoopRank with a more traditional data driven method that utilizes the integral of absolute or squared error to determine loop interaction. Canonical correlation was used to calculate the relative weight matrix in the LoopRank methodology.
It was found that both methods delivered similar results.\\ 

The LoopRank methodology was further expanded on by \cite{streicher2014eigenvector} who tested the LoopRank using transfer entropy in an attempt to get a better estimation of causality when calculating the relative wight matrix. \cite{streicher2019plant} used this methodology for incipient fault detection by calculating importance scores over multiple overlapping time regions. \\ 

\section{VISUALIZATION FOR FAULT DETECTION}

The methodologies involved in fault detection and diagnosis of industrial processes rely heavily on the visual analysis of time series data. Data is continuously generated by industrial processes, this data in its simplest form can be floating point or boolean values on a control system interface screen. This data is used by plant personnel as feedback on the current status of automated processes.

This study will focus on methods and approaches taken to visualize temporal or time series data. 

\subsection{VISUALIZATION IN GENERAL}

Some of the first books to discuss information visualization date back to 1967 with the publication of Jacques Bertin's Semiologie Graphique \cite{rees2019survey}. A survey by \cite{rees2019survey} highlighted the number of visualization books have been published each year, displayed in \ref{fig:1}.

\begin{figure}[!ht]
	
	\centering\epsfig{figure=Visualization_books_published.PNG}
	\caption{Visualization books published each year}\label{fig:1}
	
\end{figure}

The IEEE Visualization conference series aimed at sharing developments in this field started in 1990 and has contributed a large amount of data on how visualization assists other fields to better understand their data. \cite{isenberg2016vispubdata} compiled a database of these publications to assist researchers in this field to understand the history of visualization in a field and make the researcher aware of new developments trends. The papers presented through these conferences serve as valuable source to describe previous work related to visualization for the use of fault detection and diagnosis.

\subsection{VISUAL ANALYTICS}

Before designing it is important to understand the mental processes behind data analytics that are used by people to understand and gain insight into data. The work discussed in this section first attempts to describe the human aspect of of the process and then attempts to integrate the role of the machine or computer into the process.

One of the earliest  models for that describe the process of 'sensemaking' was developed by \cite{pirolli2005sensemaking}. In this model intelligence analysis or sensemaking  consists of:
\begin{enumerate}
	\item information gathering
	\item representation of the information in the form of an outline or model that aids the analysis
	\item development of insight through manipulation of the outline or model
	\item Creation of a knowledge product or direct action based on the analysis
\end{enumerate}, 
The process is visually described by Figure\ref{fig:3}.
\begin{figure}[!ht]
	\centering\epsfig{figure=Sense_making_model.PNG,scale=0.5}
	\caption{\cite{pirolli2005sensemaking} sensemaking model}\label{fig:3}
\end{figure}

The rectangles represent the flow of data and the circles represent the process flow. What is very clear from this model is that there are a lot of back loops and sets of activities that cycle around finding information and making sense of the information. The end result of the process is the generation of novel information by the analyst.

The External data sources are filtered into a smaller data set called a shoebox. Snippets are then extracted from the shoe box based on observed relationships and placed in the Evidence File. A Schema or model is then developed and verified using the Evidence file. A Hypotheses is then form based on this model with a feedback loop that search for additional support of this hypotheses. Finally there is a presentation that is first reevaluated before becoming the product of this work. This model can be used in a top-down (theory to data)  or bottom-up (data to theory) approach.

Another approach that represents a more non-linear model that might be a closer representation to how humans actually process information. \cite{klein2006making} present a model that starts of with the assumption that people always start making sense of something with some perspective, viewpoint or framework. The frame basically functions as hypothesis of how the data fits together.

These frameworks or frames actually define what counts as data and shape the data. Frames also change as we acquire data making it a two way path between a frame and data.

The first cycle that starts from the frame/ data idea is the elaboration cycle. In this cycle the frame is expanded and questioned - there could be doubt about the explanation it provides. If the doubt, possibly caused by troublesome data is explained away then the frame is preserved.

The second cycle involves a re framing process. If questioning the frame leads to its rejection, it might lead to its replacement with a better one. We could compare frames to see which one fits the data best. 

This model is displayed in Figure\ref{fig:4}.
\begin{figure}[!ht]
	\centering\epsfig{figure=Frame_data_sensemaking_model.PNG,scale=0.7}
	\caption{\cite{klein2006making} frame-data sensemaking model}\label{fig:4}
\end{figure}

This is a continuous process in which a frame is strengthened or improvement at each iteration.

Any method for turning data into knowledge requires the manipulation of the data. The analysis would start with an analysis of the data. This analysis has traditionally been done manually but with the exponential growth in the amount of data that can form part of an analysis other methods are being developed to make this step more efficient. \cite{fayyad1996data} analyzed this challenge in the emerging field of knowledge discovery in databases (KDD). In simple terms this field is concerned with development of methods and techniques for making sense of data.


One of the most noteworthy fields where these methods are applied is in the areas of astronomy. These fields contain terabytes of data contained in images. Although industrial application of data analysis contain a lot less data than this it could still be worth while studying these techniques to see if they can assist with effectively utilizing large amounts of time series data.

The goal of KDD is extracting high-level knowledge from low-level data in the context of large data sets \cite{fayyad1996data}. The KDD process is visually described by Figure \ref{fig:6}.
\begin{figure}[!ht]
	\centering\epsfig{figure=knowledge_discovery_in_databases.PNG,scale=0.8}
	\caption{\cite{fayyad1996data} steps in the knowledge discovery in databases model}\label{fig:6}
\end{figure}
The process is described by the following steps:
\begin{enumerate}
	\item Develop understanding of application domain and use prior knowledge to determine the goal of the KDD
	\item Create a dataset or subset of the available data that will be used to discover useful information
	\item Data cleaning, in this step invalid data like noise of questionable data is removed in order to prevent false deductions
	\item Data reduction and projection - finding useful features to represent the data, in this step dimensional reduction can be done to simplify the analysis
	\item Match the goals of KDD to a specific data mining method
	\item Exploratory analysis of the model and hypothesis selection, in this step the data mining method is chosen as  well as the methods that wil be used to sreach for patterns
	\item Data mining - searching for patterns in the data that would confirm the hypothesis
	\item Interpreting the mined patters
	\item Acting on the discovered knowledge, using it to solve the problem
\end{enumerate}
These steps describe a linear process but there can be movement back and forth between the steps to refine the process.


\cite{sacha2014knowledge} goes a step further to describe the process in which knowledge is generated from data by developing a model that defines how the human and computer interact. 
This model is split into two parts: 
\begin{enumerate}
	\item computer system which involves the data, visualization and analytic models
	\item human component that describes the cognitive process  associated with an analytical session
\end{enumerate}
What makes visual analytics so effective is the integration of these two parts; the human side  provides the perceptive skills, cognitive reasoning and domain knowledge while the computer side provides the computing data storage capability \cite{sacha2014knowledge}. This visual analytic model also attempts to incorporate some of the other models described earlier like the Sensemaking model \cite{pirolli2005sensemaking} and the Knowledge Discovery Process in Databases (KDD) \cite{fayyad1996data}. The complete knowledge generation model with the reference to other models is displayed in Figure \ref{fig:5}.
\begin{figure}[!ht]
	\centering\epsfig{figure=human_computer_va_model.PNG,scale=0.5}
	\caption{\cite{sacha2014knowledge} computer-human interaction in knowledge generating process}\label{fig:5}
\end{figure}


The computer part of the model has three main elements:
\begin{enumerate}
	\item Data - this includes all the data that describes the analytical problem and includes facts in any structured, semi-structured or unstructured manner. Another element to this is a term called meta data, which could be data that describes the structure of other data or data that summarizes other data.
	\item Model - This can be any descriptive measure of the data ranging from statistical properties of the data to complex data mining algorithms. This process can deliver a single number to describe the strength of an hypothesis or a complex pattern.
	\item Visualization - In this process the data is transformed into some kind of visual artifact that enables the analyst to detect relationships.
\end{enumerate}, 
The human element is also described in three main loops:
The computer part of the model has three main elements:
\begin{enumerate}
	\item Exploration loop - Findings are made based on observations of the visual analytics system, Actions or individual tasks are then performed that result in tangible  and unique responses from the visual analytics system. 
	\item Verification loop - Insight is gained from interpreting Findings, often with previous domain knowledge to generate units of information. A Hypothesis is then formed based on these Insights and tested in the visual analytics system through Actions.
	\item Knowledge - The final part is to build up confidence in the Insights from the data and Hypotheses tested on the data. If  patterns or stories told by the data makes sense and can be used to answer the questions from the visual analytic process then Knowledge is generated. This Knowledge is added to the domain knowledge of the analyst to ask more questions and generate more Knowledge.
\end{enumerate}
This model proposed by \cite{sacha2014knowledge} again highlights the iterative process of visual analytics and how feedback loops are used in every element of the model.


\subsection{VALUE OF RECORD KEEPING IN VISUALIZATION}

\cite{mahyar2010closer} conducted a experiment to study the visual analytics process and the role of record keeping in this process. This study also elaborated on key elements for the design of a collaborative visual analysis tool. The study required groups to complete tasks given a certain data set using computer aided visual analytics.
Actions taken during visual analytics were grouped into four phases:
\begin{enumerate}
	\item Problem definition - understanding what needs to be solved
	\item Visualization - generating visual artifacts
	\item Analysis - most complex phase that requires examining of charts and making sense of them combined with other data
	\item Dissemination - compiling findings in a report
\end{enumerate}
The findings from the study conducted by \cite{mahyar2010closer} confirmed observations by other researchers about the non-linear temporal order of activities. While completing the tasks users would move through the phases in a variety of orders. The visualization and analysis phases were strongly interrelated, the users moved back and forth between these two phases much more than the other phases.

\cite{mahyar2010closer} generated a very visual model of these four phases with key elements noted in each phase:
\begin{figure}[!ht]
	\centering\epsfig{figure=Visual_Analytics_Model.PNG,scale=0.5}
	\caption{Visualization books published each year}\label{fig:2}
\end{figure}

Record keeping involves the saving of visual artifacts and notes. In most cases a visual artifact can convey a complex idea more effectively than notes. Visual artifacts were saved for two main reasons; using them in reporting and using them to enhance the analysis process.\cite{mahyar2010closer} recommends that visual analytic systems provide functionality to save visual artifacts like charts for later use and comparison. Note taking also plays a critical role in all phases of the process. The notes high level content could be divided into two categories; findings and cues. Findings are a results of mathematical calculations, statistical operations or decisions and outcomes of the analysis process, they could also take to form of saved charts. Cues can be anything that is not directly extracted from the visualizations that could assist the user to framing or understanding the task at hand. A larger proportion of findings were recorded in the analysis phase while the cues were largely taken during the visualization phase. The study concluded that record keeping is used intensively by data analysts.

\cite{saraiya2006insight} completed a longitudinal study of a bioinformatics data set analysis to gain understanding of the entire analysis process from raw data to insights. Visual analytics can be described as a process wherein user gain insight into complex data. The study highlighted how analysts gather and build insights over long periods of time and connecting the data to extensive domain knowledge and expertise. It is emphasized that analyst spend a significant amount of time on manual manipulation of data and this creates a very important need for the visualization tool to be interactive. Inferior visualizations with interaction were preferred over superior static visualizations.

Visualization plays a critical role in these data driven problem solving techniques. \cite{elmqvist2004animated} has shown that traditional methods like directed-acyclic graphs and Hasse diagrams can display casual links in an intuitive way but becomes less suitable when the number of nodes and interactions in a system grow. The use of two novel visualization techniques called Growing Squares and Growing Polygons demonstrate the use of animations, colors and patterns to better visualize casual relations.

Another approach to improve visualization of casual relations would be to adapt novel techniques used in other fields, similar to how PageRank was adapted to be used as LoopRank in industrial processes. Process monitoring and fault detection of batch processes has received attention in recent years. The extensive use of batch processing in the production of high value products makes it a important to ensure that batch runs are completed with a high level of success. \cite{wang2018geometric} developed a novel approach to process monitoring and fault detection in batch processes by utilizing a technique they termed time-explicit Kiviat diagrams. The core idea behind this technique is based on Multi-way Principle Component Analysis that facilitates batch-wise unfolding of the three dimensional (Time*Variables*Batches) batch data matrix into a two dimensional matrix that captures the variation across batches. The type of data required to warrant the use of this technique is very similar to the data generated by multiple time region analysis in the LoopRank analysis done by \cite{streicher2019plant}. Each batch can be seen as a time region in the LoopRank analysis and the analysis can be seen as the completion of multiple batch runs.

Equation \eqref{fig:example} shows an example of a numbered equation.
\begin{equation}
  \Delta w_{i,j} = \alpha a_i \Delta[j]  \label{eqn:example}
\end{equation}

\begin{align}
  \m{QX} \m{a} & = \m{Qy} \notag \\
  \therefore \quad (\m{QX})^T \m{QX} \m{a} & = (\m{QX})^T \m{Qy} \notag \\
  \therefore \quad \m{a} & = \left( (\m{QX})^T \m{QX} \right)^{-1} (\m{QX})^T \m{Qy} \notag \\
        & = \left( \mt{X} \mt{Q} \m{QX} \right)^{-1} \mt{X} \mt{Q} \m{Qy} \notag \\
        & = \left( \mt{X} \m{W} \m{X} \right)^{-1} \mt{X} \m{W} \m{y}
          \label{equ:weightedleastsquares}
\end{align}

\subsection{A subsection with a long heading that continues from the
  first line onto the second line see also table of contents}


\subsubsection{Testing deeper nested section headers}



\ref{fig:examplea} and \ref{fig:exampleb}.
\begin{figure}[!ht]
  \begin{minipage}[b]{.5\linewidth}
  \centering\epsfig{figure=figure1.eps}
  \subcaption{A subfigure}\label{fig:examplea}
  \end{minipage}%
  \begin{minipage}[b]{.5\linewidth}
  \centering\epsfig{figure=figure1.eps,angle=-90}
  \subcaption{Another subfigure}\label{fig:exampleb}
  \end{minipage}
  \caption{This is a figure caption. In the example \subref{fig:exampleb}
  is a rotated version of \subref{fig:examplea}.}\label{fig:example}
\end{figure}

Third theme of literature study third theme of literature study third
theme of literature study third theme of literature study.



Table \ref{fig:example} shows an example of a table.
\begin{center}
\begin{table}[!ht]
\caption{This is a table caption}
\label{fig:table}
\begin{tabular}{ !{\vrule width 1.1pt}
                c!{\vrule width 1pt}
                c!{\vrule width 1pt}
                c!{\vrule width 1pt}
                p{8cm}!{\vrule width 1pt}}
  \noalign{\hrule height 1pt}
  \cellcolor[gray]{0.9} \textbf{Column 1} &
  \cellcolor[gray]{0.9} \textbf{Column 2} &
  \cellcolor[gray]{0.9} \textbf{Column 3} &
  \cellcolor[gray]{0.9} \textbf{Column 4}
  \\ \noalign{\hrule height 1pt}
   123     &  abc & 123 & abc
  \\ \hline
   123     &  abc & 123 & abc
  \\ \hline
           &      &  & abc
  \\ \noalign{\hrule height 1pt}
\end{tabular}
\end{table}
\end{center}



%% End of File.
