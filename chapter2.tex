%%
%%  Department of Chemical Engineering
%%  MEng Dissertation - Chapter 2
%%  Copyright (C) 2011-2016 University of Pretoria.
%%

\chapter{LITERATURE STUDY}

\section{CHAPTER OBJECTIVES}

This chapter explores the literature in the field of disturbance detection and diagnosis in control of industrial processes. It then elaborates of visualization techniques and how they could be used to aid the in the fault detection and diagnosis process.

\section{FAULT DETECTION AND ANALYSIS}

Disturbances in single control loops are fairly straight forward to solve but disturbances that propagate through a complex system on a plant wide scale are much harder to pin point. One of the goals of process control is to shift process variability to places like buffer tanks and plant utilities. In modern industrial processes these options have become limited as inventory is usually kept to a minimum and the use of recycle stream for heat integration has introduced extra sources complex interaction \cite{thornhill2007advances}.

When considering which type of disturbances to analyze, the time frame of the disturbance can be used to distinguish between different types of disturbances. \cite{thornhill2007advances} classified the types of disturbances in a plant wide setting into three main categories with regards to timescales:
\begin{enumerate}
	\item slowly-developing, i.e. fouling of a heat exchanger
	\item persistent and dynamic
	\item abrupt, i.e. a compressor trip
\end{enumerate}
The second item, persistent and dynamic disturbances that propagate through an entire system on a continuous basis is of interest for this study.

New data driven methods that apply concepts from other fields have started to show some promising results. One of these methods is the use of the PageRank algorithm \cite{bryan200625} employed by google on the ranking of control loops based on the degree of interaction.\cite{farenzena2009looprank} has developed a methodology called LoopRank with the aim of prioritizing control loop maintenance by highlighting loops that have a higher correlation with other loops. In this methodology a relative weight matrix is compiled using partial correlation between input and output variables. The importance scores of each variable is then determined by calculating the normalized eigenvector of the relative weight matrix.\\

\subsection{PAGERANK DERIVATION}

The ranking algorithm developed by \cite{bryan200625} started with the idea of ranking web sites depending on how many times they are referenced by other websites.

Consider a group of 4 websites that contain references to each other in the manner indicated in Figure \ref{fig:7}. References are indicated by arrows.\\
\begin{figure}[H]
	\centering\epsfig{figure=webpages.PNG,scale=0.5}
	\caption{web pages with references}\label{fig:7}
\end{figure}
An initial idea would be that a page that has more references or back links is more important. Let $x_{k}$ be the importance of page k. If this method is followed then $x_{3} = 3$ would be the most important web page because it has the most references.

This initial ranking method indicates that $x_{3}$ is important, but it doesn't take into account the importance that $x_{3}$ passes on to $x_{1}$ because it is the only page referenced by $x_{3}$.

Another approach would be to let back links from important pages count more than back links from less important pages. If a page k has a back link from page j, then the score given to this back link can be calculated as $x_{k} = \frac{x_{j}}{n_{j}}$ where $n_{j}$ is the total number of references made by page j. The total score for page $x_{k}$ is then calculated by the sum of all back links to it adjusted for the number of references made. Applying this new method the importance score for each node is calculated:
\newline $x_{1} = \frac{x_{3}}{1} + \frac{x_{4}}{2} $ 
\newline $x_{2} = \frac{x_{1}}{3} $
\newline $x_{3} = \frac{x_{1}}{3} + \frac{x_{2}}{2} + \frac{x_{4}}{2} $
\newline $x_{4} = \frac{x_{1}}{3} + \frac{x_{2}}{2} $
\newline These equations can be written in a matrix form, which will be defined as the link matrix A:
\newline 
\[A=\begin{bmatrix}
0           & 0           & 1 & \frac{1}{2} \\
\frac{1}{3} & 0           & 0 & 0           \\
\frac{1}{3} & \frac{1}{2} & 0 & \frac{1}{2} \\
\frac{1}{3} & \frac{1}{2} & 0 & 0
\end{bmatrix}\]
\newline Each row in the matrix A represents a page or node while each column  represents the importance score from another node. For example row one represents node 1 and it has a back link from node 3 with an importance score of 1 and a back link from node 4 with a importance score of $\frac{1}{2}$.

if $x = \begin{bmatrix} x_{1} & x_{2} & x_{3} & x_{4} \end{bmatrix} ^{T}$ then the problem can be defined as the eigenvector problem:
\newline $Ax = x \lambda$ 
\newline with x being an eigenvector for A with en eigenvalue of 1
\newline if A is a $n\times n$ matrix, then a nonzero vector x is an eigenvector if $Ax = \lambda x$ for some scalar $\lambda$. The eigenvector x that solves this equation is then calculated to be:
\newline $x \approx \begin{bmatrix} 0.387 & 0.129 & 0.290 & 0.194\end{bmatrix}^{T}$
which shows that node one is in fact more important than node three.

\subsection{LOOPRANK ADAPTION}

\cite{farenzena2009looprank} developed a method called LoopRank which is based on the PageRank algorithm to prioritize control loop maintenance. Modern industrial sites have a large number of control loops which becomes challenging to maintain effectively given limited technical resources. 

The LoopRank method can be summarized in five steps:
\begin{enumerate}
	\item Collect time series data of control loop parameters
	\item Calculate the partial correlation between all the variables in the data set. This information is used to build a relative weight matrix A or link matrix as referred to in the PageRank algorithm
	\item Compute the LoopRank based on the relative weight matrix A using the eigenvector centrality theorem
	\item Multiply the result for each loop with a pre-defined weight that is based on the criticality of the loop. This weight is assigned heuristically depending on loop type and profitability
	\item Normalize the results to express the score for each loop as a percentage
\end{enumerate}

\cite{farenzena2009looprank} demonstrated the efficacy of LoopRank by testing it on an industrial case study in which it successfully flagged the most critical control loops.

\subsection{LOOPRANK DEVELOPMENT}

\cite{rahman2010new} compared LoopRank with a more traditional data driven method that utilizes the integral of absolute or squared error to determine loop interaction. Canonical correlation was used to calculate the relative weight matrix in the LoopRank methodology. It was found that both methods delivered similar results.\\ 
The LoopRank methodology was further expanded on by \cite{streicher2014eigenvector} who tested the LoopRank using transfer entropy in an attempt to get a better estimation of causality when calculating the relative weight matrix. \cite{streicher2019plant} used this methodology for incipient fault detection by calculating importance scores over multiple overlapping time regions. \\ 

\section{VISUALIZATION FOR FAULT DETECTION}

The methodologies involved in fault detection and diagnosis of industrial processes rely heavily on the visual analysis of time series data. Data is continuously generated by industrial processes, this data in its simplest form can be floating point or boolean values on a control system interface screen. This data is used by plant personnel as feedback on the current status of automated processes.

This study will focus on methods and approaches taken to visualize temporal or time series data. 

\subsection{VISUALIZATION IN GENERAL}

Some of the first books to discuss information visualization date back to 1967 with the publication of Jacques Bertin's Semiologie Graphique \cite{rees2019survey}. A survey by \cite{rees2019survey} highlighted the number of visualization books have been published each year, displayed in \ref{fig:1}.

\begin{figure}[!ht]
	
	\centering\epsfig{figure=Visualization_books_published.PNG}
	\caption{Visualization books published each year}\label{fig:1}
	
\end{figure}

The IEEE Visualization conference series aimed at sharing developments in this field started in 1990 and has contributed a large amount of data on how visualization assists other fields to better understand their data. \cite{isenberg2016vispubdata} compiled a database of these publications to assist researchers in this field to understand the history of visualization in a field and make the researcher aware of new developments trends. The papers presented through these conferences serve as valuable source to describe previous work related to visualization for the use of fault detection and diagnosis.

\subsection{VISUAL ANALYTICS}

Before designing it is important to understand the mental processes behind data analytics that are used by people to understand and gain insight into data. The work discussed in this section first attempts to describe the human aspect of of the process and then attempts to integrate the role of the machine or computer into the process.

One of the earliest  models for that describe the process of 'sensemaking' was developed by \cite{pirolli2005sensemaking}. In this model intelligence analysis or sensemaking  consists of:
\begin{enumerate}
	\item Information gathering
	\item Representation of the information in the form of an outline or model that aids the analysis
	\item Development of insight through manipulation of the outline or model
	\item Creation of a knowledge product or direct action based on the analysis
\end{enumerate}
The process is visually described by Figure\ref{fig:3}.
\begin{figure}[!ht]
	\centering\epsfig{figure=Sense_making_model.PNG,scale=0.5}
	\caption{\cite{pirolli2005sensemaking} sensemaking model}\label{fig:3}
\end{figure}

The rectangles represent the flow of data and the circles represent the process flow. What is very clear from this model is that there are a lot of back loops and sets of activities that cycle around finding information and making sense of the information. The end result of the process is the generation of novel information by the analyst.

The External data sources are filtered into a smaller data set called a shoebox. Snippets are then extracted from the shoe box based on observed relationships and placed in the Evidence File. A Schema or model is then developed and verified using the Evidence file. A Hypotheses is then form based on this model with a feedback loop that search for additional support of this hypotheses. Finally there is a presentation that is first reevaluated before becoming the product of this work. This model can be used in a top-down (theory to data)  or bottom-up (data to theory) approach.

Another approach that represents a more non-linear model that might be a closer representation to how humans actually process information. \cite{klein2006making} present a model that starts of with the assumption that people always start making sense of something with some perspective, viewpoint or framework. The frame basically functions as hypothesis of how the data fits together.

These frameworks or frames actually define what counts as data and shape the data. Frames also change as we acquire data making it a two way path between a frame and data.

The first cycle that starts from the frame/ data idea is the elaboration cycle. In this cycle the frame is expanded and questioned - there could be doubt about the explanation it provides. If the doubt, possibly caused by troublesome data is explained away then the frame is preserved.

The second cycle involves a re framing process. If questioning the frame leads to its rejection, it might lead to its replacement with a better one. We could compare frames to see which one fits the data best. 

This model is displayed in Figure\ref{fig:4}.
\begin{figure}[!ht]
	\centering\epsfig{figure=Frame_data_sensemaking_model.PNG,scale=0.7}
	\caption{\cite{klein2006making} frame-data sensemaking model}\label{fig:4}
\end{figure}

This is a continuous process in which a frame is strengthened or improvement at each iteration.

Any method for turning data into knowledge requires the manipulation of the data. The analysis would start with an analysis of the data. This analysis has traditionally been done manually but with the exponential growth in the amount of data that can form part of an analysis other methods are being developed to make this step more efficient. \cite{fayyad1996data} analyzed this challenge in the emerging field of knowledge discovery in databases (KDD). In simple terms this field is concerned with development of methods and techniques for making sense of data.


One of the most noteworthy fields where these methods are applied is in the areas of astronomy. These fields contain terabytes of data contained in images. Although industrial application of data analysis contain a lot less data than this it could still be worth while studying these techniques to see if they can assist with effectively utilizing large amounts of time series data.

The goal of KDD is extracting high-level knowledge from low-level data in the context of large data sets \cite{fayyad1996data}. The KDD process is visually described by Figure \ref{fig:6}.
\begin{figure}[!ht]
	\centering\epsfig{figure=knowledge_discovery_in_databases.PNG,scale=0.8}
	\caption{\cite{fayyad1996data} steps in the knowledge discovery in databases model}\label{fig:6}
\end{figure}
The process is described by the following steps:
\begin{enumerate}
	\item Develop understanding of application domain and use prior knowledge to determine the goal of the KDD
	\item Create a data set or subset of the available data that will be used to discover useful information
	\item Data cleaning, in this step invalid data like noise of questionable data is removed in order to prevent false deductions
	\item Data reduction and projection - finding useful features to represent the data, in this step dimensional reduction can be done to simplify the analysis
	\item Match the goals of KDD to a specific data mining method
	\item Exploratory analysis of the model and hypothesis selection, in this step the data mining method is chosen as  well as the methods that will be used to search for patterns
	\item Data mining - searching for patterns in the data that would confirm the hypothesis
	\item Interpreting the mined patters
	\item Acting on the discovered knowledge, using it to solve the problem
\end{enumerate}
These steps describe a linear process but there can be movement back and forth between the steps to refine the process.


\cite{sacha2014knowledge} goes a step further to describe the process in which knowledge is generated from data by developing a model that defines how the human and computer interact. 
This model is split into two parts: 
\begin{enumerate}
	\item computer system which involves the data, visualization and analytic models
	\item human component that describes the cognitive process  associated with an analytical session
\end{enumerate}
What makes visual analytics so effective is the integration of these two parts; the human side  provides the perceptive skills, cognitive reasoning and domain knowledge while the computer side provides the computing data storage capability \cite{sacha2014knowledge}. This visual analytic model also attempts to incorporate some of the other models described earlier like the Sensemaking model \cite{pirolli2005sensemaking} and the Knowledge Discovery Process in Databases (KDD) \cite{fayyad1996data}. The complete knowledge generation model with the reference to other models is displayed in Figure \ref{fig:5}.
\begin{figure}[!ht]
	\centering\epsfig{figure=human_computer_va_model.PNG,scale=0.5}
	\caption{\cite{sacha2014knowledge} computer-human interaction in knowledge generating process}\label{fig:5}
\end{figure}


The computer part of the model has three main elements:
\begin{enumerate}
	\item Data - this includes all the data that describes the analytical problem and includes facts in any structured, semi-structured or unstructured manner. Another element to this is a term called meta data, which could be data that describes the structure of other data or data that summarizes other data.
	\item Model - This can be any descriptive measure of the data ranging from statistical properties of the data to complex data mining algorithms. This process can deliver a single number to describe the strength of an hypothesis or a complex pattern.
	\item Visualization - In this process the data is transformed into some kind of visual artifact that enables the analyst to detect relationships.
\end{enumerate}, 
The human element is also described in three main loops:
The computer part of the model has three main elements:
\begin{enumerate}
	\item Exploration loop - Findings are made based on observations of the visual analytics system, Actions or individual tasks are then performed that result in tangible  and unique responses from the visual analytics system. 
	\item Verification loop - Insight is gained from interpreting Findings, often with previous domain knowledge to generate units of information. A Hypothesis is then formed based on these Insights and tested in the visual analytics system through Actions.
	\item Knowledge - The final part is to build up confidence in the Insights from the data and Hypotheses tested on the data. If  patterns or stories told by the data makes sense and can be used to answer the questions from the visual analytic process then Knowledge is generated. This Knowledge is added to the domain knowledge of the analyst to ask more questions and generate more Knowledge.
\end{enumerate}
This model proposed by \cite{sacha2014knowledge} again highlights the iterative process of visual analytics and how feedback loops are used in every element of the model.


\subsection{VALUE OF RECORD KEEPING IN VISUALIZATION}

\cite{mahyar2010closer} conducted a experiment to study the visual analytics process and the role of record keeping in this process. This study also elaborated on key elements for the design of a collaborative visual analysis tool. The study required groups to complete tasks given a certain data set using computer aided visual analytics.
Actions taken during visual analytics were grouped into four phases:
\begin{enumerate}
	\item Problem definition - understanding what needs to be solved
	\item Visualization - generating visual artifacts
	\item Analysis - most complex phase that requires examining of charts and making sense of them combined with other data
	\item Dissemination - compiling findings in a report
\end{enumerate}
The findings from the study conducted by \cite{mahyar2010closer} confirmed observations by other researchers about the non-linear temporal order of activities. While completing the tasks users would move through the phases in a variety of orders. The visualization and analysis phases were strongly interrelated, the users moved back and forth between these two phases much more than the other phases.

\cite{mahyar2010closer} generated a very visual model of these four phases with key elements noted in each phase:
\begin{figure}[!ht]
	\centering\epsfig{figure=Visual_Analytics_Model.PNG,scale=0.5}
	\caption{Visualization books published each year}\label{fig:2}
\end{figure}

Record keeping involves the saving of visual artifacts and notes. In most cases a visual artifact can convey a complex idea more effectively than notes. Visual artifacts were saved for two main reasons; using them in reporting and using them to enhance the analysis process.\cite{mahyar2010closer} recommends that visual analytic systems provide functionality to save visual artifacts like charts for later use and comparison. Note taking also plays a critical role in all phases of the process. The notes high level content could be divided into two categories; findings and cues. Findings are a results of mathematical calculations, statistical operations or decisions and outcomes of the analysis process, they could also take to form of saved charts. Cues can be anything that is not directly extracted from the visualizations that could assist the user to framing or understanding the task at hand. A larger proportion of findings were recorded in the analysis phase while the cues were largely taken during the visualization phase. The study concluded that record keeping is used intensively by data analysts.

\cite{saraiya2006insight} completed a longitudinal study of a bioinformatics data set analysis to gain understanding of the entire analysis process from raw data to insights. Visual analytics can be described as a process wherein user gain insight into complex data. The study highlighted how analysts gather and build insights over long periods of time and connecting the data to extensive domain knowledge and expertise. It is emphasized that analyst spend a significant amount of time on manual manipulation of data and this creates a very important need for the visualization tool to be interactive. Inferior visualizations with interaction were preferred over superior static visualizations.

\subsection{VISUALIZATION TOOLS}

\cite{barnard2015usability} performed a comparison of visualization tools in the development of a tool that can be used for data analysis at CERN. The goal of this tool was to enable scientist to observe a data set as a whole in order to make better decisions of what variable pairs and time frames to analyze during test campaigns.

JavaScript and Python Libraries were investigated and evaluated. D3 and Dygraphs were both discussed as JavaScript libraries that can be used for plotting. Matplotlib was discussed as a Python library that is based on the plotting functionalities of MATLAB. Two other Python libraries, Bokeh and Mpld3 were discussed with specific attention to their capabilities to generate HTML which enables them to easily present graphs on a web browser. Furthermore Mpld3 can integrate Matplotlob plots using D3 to render them to a web page.

\cite{barnard2015usability} did a detailed comparison of Bokeh and Mpld3 which identified Bokeh as their preferred choice. Comparing key aspects like loading time, rendering time and features Bokeh was faster and had more actively developed libraries.

\cite{fahad2018big} did a comparison of data visualization systems with the specific focus on big data applications. Tools were categorized into coding and zero coding tools with graphical user interfaces. 

Within the coding category two languages were discussed; Python and R. Within Python multiple libraries were analyzed for visualization, including Bokeh, Altair, Seaborn, ggplot and Pygal.
The zero coding category included Tableau, Infogram, Charblocks, Datawrapper, Plotly, RAW and Visual. It's important to note that of these zero coding tools Plotly is also a Python library used by the data science community.

\cite{ali2016big} also completed an overview of tools used for big data visualization and the challenges faced with attempting to visualization large amounts of data. The tools were compared using the following criteria:
\begin{enumerate}
	\item Open Source
	\item Integration with popular data sources like Hadoop Hive or Google Analytics
	\item Interactive visualization
	\item MOOCS; tutorials available for online learning of the tool
	\item API; can the services of the tool be embedded
\end{enumerate}
Five tools were analyzed using this criteria:
\begin{enumerate}
	\item Tableau
	\item Power BI
	\item Plotly
	\item Gephi
	\item Excel 2016
\end{enumerate}
A summary of the results is displayed in 
Table \ref{fig:tool_comparison} shows an example of a table.
\begin{center}
	\begin{table}[H]
		\caption{This is a table caption}
		\label{fig:tool_comparison}
		\begin{tabular}{ 
				!{\vrule width 1.1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}
				c!{\vrule width 1pt}}
				%p{8cm}!{\vrule width 1pt}}
			\noalign{\hrule height 1pt}
			\cellcolor[gray]{0.9} \textbf{} &
			\cellcolor[gray]{0.9} \textbf{Open source} &
			\cellcolor[gray]{0.9} \textbf{Integration} &
			\cellcolor[gray]{0.9} \textbf{Interactive} &
			\cellcolor[gray]{0.9} \textbf{MOOCS} &
			\cellcolor[gray]{0.9} \textbf{API}
			\\ \noalign{\hrule height 1pt}
			Tableau & N & Y & Y & Y & Y
			\\ \hline
			Power BI & N & Y & Y & Y & Y
			\\ \hline
			Plotly & Y & N & Y & Y & Y
			\\ \hline
			Gephi & Y & N & Y & Y & Y
			\\ \hline
			Tableau & N & Y & Y & Y & Y
			\\ \noalign{\hrule height 1pt}
		\end{tabular}
	\end{table}
\end{center}

One of the biggest challenges with modern visualization tools is the latency created by processing large data sets \cite{ali2016big}. This can be addressed by using pre-computed data, paralizing data processing and rendering and using predictive middelware.

\cite{caldarola2017big} completed a comprehensive survey of tools used in Big Data applications. The concept of Big Data has different dimensions, one model that attempts to described these dimensions is widely known as 3Vs. The 3Vs stand for Volume, Velocity and Variety.  Each of these dimensions pose its own unique challenges in the data management process.

The analysis and visualization of data was represented in a diagram that distinguishes between four different types of software solutions \cite{caldarola2017big}. The diagram displayed in Figure \ref{fig:10} depicts the different types according to the Volume and Variety challenges in the 3Vs model. 
\begin{figure}[H]
	\centering\epsfig{figure=big_data_visualization_categories.PNG,scale=0.9}
	\caption{Big data visualization solutions}\label{fig:10}
\end{figure}
Differentiation is also done according to four main categories that describe the goals of visualization. The first goal would be the clear and effective communication of information to the user with the creation and study of the visual representation of data. The Information Visualization category is created to describe this goal. The second goal diverges from the first one in terms of spatial representation. If spatial representation is given to the visualization because of the intrinsic spatial layout of data like a flow simulation in a 3D space, then this can be seen as Scientific Visualization. These two categories can be further divided according to the dimension of volume and variety of data. In the end these four categories chosen by \cite{caldarola2017big} to divide software are:
\begin{enumerate}
	\item business intelligent tools for data mining of heterogeneous data
	\item scientific visualization tools focused that focus on specific spatial representations using larger data sets
	\item data visualization tools used for visual exploration
	\item information visualization tools used for interactive visual exploration of data
\end{enumerate}
\cite{caldarola2017big} analyses 35 different software solutions in these four categories. What is interesting to note is that Plotly is well described in both the data and information visualization categories which implies that does well at handling big data challenges with variety and volume.





Visualization plays a critical role in these data driven problem solving techniques. \cite{elmqvist2004animated} has shown that traditional methods like directed-acyclic graphs and Hasse diagrams can display casual links in an intuitive way but becomes less suitable when the number of nodes and interactions in a system grow. The use of two novel visualization techniques called Growing Squares and Growing Polygons demonstrate the use of animations, colors and patterns to better visualize casual relations.

Another approach to improve visualization of casual relations would be to adapt novel techniques used in other fields, similar to how PageRank was adapted to be used as LoopRank in industrial processes. Process monitoring and fault detection of batch processes has received attention in recent years. The extensive use of batch processing in the production of high value products makes it a important to ensure that batch runs are completed with a high level of success. \cite{wang2018geometric} developed a novel approach to process monitoring and fault detection in batch processes by utilizing a technique they termed time-explicit Kiviat diagrams. The core idea behind this technique is based on Multi-way Principle Component Analysis that facilitates batch-wise unfolding of the three dimensional (Time*Variables*Batches) batch data matrix into a two dimensional matrix that captures the variation across batches. The type of data required to warrant the use of this technique is very similar to the data generated by multiple time region analysis in the LoopRank analysis done by \cite{streicher2019plant}. Each batch can be seen as a time region in the LoopRank analysis and the analysis can be seen as the completion of multiple batch runs.

%Equation \eqref{fig:example} shows an example of a numbered equation.
%\begin{equation}
%  \Delta w_{i,j} = \alpha a_i \Delta[j]  \label{eqn:example}
%\end{equation}
%
%\begin{align}
%  \m{QX} \m{a} & = \m{Qy} \notag \\
%  \therefore \quad (\m{QX})^T \m{QX} \m{a} & = (\m{QX})^T \m{Qy} \notag \\
%  \therefore \quad \m{a} & = \left( (\m{QX})^T \m{QX} \right)^{-1} (\m{QX})^T \m{Qy} \notag \\
%        & = \left( \mt{X} \mt{Q} \m{QX} \right)^{-1} \mt{X} \mt{Q} \m{Qy} \notag \\
%        & = \left( \mt{X} \m{W} \m{X} \right)^{-1} \mt{X} \m{W} \m{y}
%          \label{equ:weightedleastsquares}
%\end{align}

%\subsection{A subsection with a long heading that continues from the
%  first line onto the second line see also table of contents}
%
%\subsubsection{Testing deeper nested section headers}

%\ref{fig:examplea} and \ref{fig:exampleb}.
%\begin{figure}[!ht]
%  \begin{minipage}[b]{.5\linewidth}
%  \centering\epsfig{figure=figure1.eps}
%  \subcaption{A subfigure}\label{fig:examplea}
%  \end{minipage}%
%  \begin{minipage}[b]{.5\linewidth}
%  \centering\epsfig{figure=figure1.eps,angle=-90}
%  \subcaption{Another subfigure}\label{fig:exampleb}
%  \end{minipage}
%  \caption{This is a figure caption. In the example \subref{fig:exampleb}
%  is a rotated version of \subref{fig:examplea}.}\label{fig:example}
%\end{figure}



%% End of File.
