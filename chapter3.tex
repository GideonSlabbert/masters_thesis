%%
%%  Department of Electrical, Electronic and Computer Engineering
%%  MEng Dissertation / PhD Thesis - Chapter 3
%%  Copyright (C) 2011-2016 University of Pretoria.
%%

\chapter{METHODS}

\section{CHAPTER OVERVIEW}

This chapter will describe the process taken to develop a visual analytic system that will support the FDD process.

\section{CONCEPTUAL DESIGN}

The sources referenced in Chapter 2 describes various different ways of how to approach FDD and what methods work best to structure the FDD process. A common idea in most of these sources is the need for data manipulation or transformation. The key to obtaining an understanding that there is a problem and what is causing it is to  view data in a certain context. This context needs to be created by interacting with the data. 

Almost all of the literature sources highlight a need for a structured approach with logical steps that lead to a final goal. \cite{venkatasubramanian2003review} describes a four step transformation process that classifies each step as a different data space; measurement, feature, decision and class. \cite{pirolli2005sensemaking} proposes a sensemaking model with four key steps: Information gathering, representation of information in a model, development of insight through manipulation of the model and finally creation of knowledge through analysis. \cite{fayyad1996data} describes a five step process that includes data selection, processing, transformation, mining and finally interpretation or evaluation. \cite{mahyar2010closer} proposes a structured approach to visual analytics that focuses of problem definition, visualization, analysis and reporting.

Based on the findings of all these sources it is proposed to also divide a new FDD tool into four distinct steps. The first and most important step would be the analysis of raw data. Before any complex transformations can be done it is important to understand what actual data is available and what the limitations are thereof. The human brain is very good at recognizing patterns without necessarily quantifying the model that is uses to identify the pattern. It is also important to understand the limitations imposed by the frequency of the data used. If the data is of a very low frequency it might not be feasible to identify faults that are caused by high frequency failures which is common when the failure is related to a instrument failure in industrial processes. There might also be differences in the frequency of different parameters in the data set. 

Process Information Management Systems (PIMS) and Laboratory Information Management Systems (LIMS) collect and stores time series data in vastly different frequencies. For example a process data point might be available at minute intervals but a critical process specification like a viscosity analysis might only be available every eight hours. Furthermore LIMS data might also pose a challenge in terms of time shifts. The manual nature of how some lab samples are collected might cause a time shift between when the sample was taken and when the analysis was completed. In practice it can be very difficult to ensure that this time shift is consistent and also how to determine what the exact time shift is.

The raw data should not only be time series data but any other data that can be used to create context. As indicated by \cite{thornhill2007advances} and \cite{yim2006using} any information regarding plant topology can provide valuable insight into the casual links between variables. This could potentially simplify any models developed and avoid nonsensical conclusions. Although the XML schema discussed by  \cite{thornhill2007advances} provides an exciting prospect in itself there is no open source version of such a application currently available. For this reason this specific schema will not be incorporated into the tool under development but the idea can still be used. Any visual or diagram of the process that generated the time series data must be included in the first step of the FDD process. Even if the casual links are specified manually this still improves greatly on assuming all parameters can be correlated to each other. Another source of additional data that is available on modern control system is event data. Event data is a valuable source fo meta data that could include alarm data, changes made to control system parameters by operators or any other predefined alerts that would be logged based on the state of a variable, i.e. the changing state of a binary variable to indicate the state of a electrical motor.

Another aspect of the first step in the proposed process is to include the ability to incorporate note taking into the raw data analysis. This ensures that any information gathered by one user can be built on by others to further the discovery process. As explained by \cite{mahyar2010closer} these notes can be in the form of findings and cues, where the findings can be as a results calculations which might only be related to one of the following steps and cues can be anything that is useful, for example an observation of the casual link identified by using the process topology.

The final thread that must be woven into the entire process is the ability to interact with the data that is available. As highlighted by [20] it is more important to have a highly interactive tool than a very accurate tool because this will encourage the engagement of the user. An engaged user will spend more time to guide the FDD process and provide valuable input into it.

The second step in the FDD process should be where the data is transformed in such a way as to highlight the most important information. This information would be the signs of a fault occurring or that a fault has resulted in some change in how the variables relate to each other. Data transformation and data mining is mentioned as key part of the study done by \cite{fayyad1996data} into knowledge databases. The study by \cite{venkatasubramanian2003review} calls this the feature space where certain features of the underlining data set are brought forward where they can be analyzed in more detail. \cite{pirolli2005sensemaking} describes this as representing information in a model, a model is something that is created to help in the understanding of how variables in a complex system are related to each other.

In this second step the most customization mst be allowed to the user to specify new methods of transforming data. There will always be new and creative ways to digest data and reduce dimensionality. For the purposes of this demonstration the tool called FaultMap that was developed by \cite{streicher2019plant} will be used as the data transformation engine. This will demonstrate how to perform a FDD analysis of a data set by following the previously explained algorithm to highlight faults and monitor how they develop over time.

The third step in the FDD process will be a space where the model or transformation developed in the second step can be modified or adjusted to test certain hypothesis. It is in this step that a decision is taken as to if the transformation is detecting a true fault. \cite{venkatasubramanian2003review} aptly calls this the decision space. \cite{pirolli2005sensemaking} describes this as development of insight through manipulation fo the model. Only when the user can see how the model changes can they truly start to internalize the effect of the model on the raw data and start to gain real understanding of what the effect of the transformation truly was. \cite{mahyar2010closer} describes this point as the analysis stage which ties in well with the idea of getting a better understanding of the end result.

The fourth and final step is called the classification space by \cite{venkatasubramanian2003review}, in this space knowledge is created through the analysis performed in the previous step. Interpretation and evaluation of the model is done and a final report is compiled of the findings of the study. This step can be very intricately intertwined with step three, but it is still important to distinguish this as the step where the final result of the FDD process is generated. this final result could then be documented and tested on a new data set or it could be used to improve any of the previous steps by modifying the transformation technique used in step two. Finally it could be used to allow the user to respond to fault conditions in the process and improve the process stability.




%% End of File.
